{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-11-04T15:48:19.33009Z","iopub.status.busy":"2022-11-04T15:48:19.329789Z","iopub.status.idle":"2022-11-04T15:48:24.46454Z","shell.execute_reply":"2022-11-04T15:48:24.463473Z","shell.execute_reply.started":"2022-11-04T15:48:19.330058Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n","import plotly.express as px\n","from plotly.offline import init_notebook_mode\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","from tqdm import tqdm\n","from nltk.stem import WordNetLemmatizer\n","import spacy\n","\n","nltk.download(\"omw-1.4\")\n","tqdm.pandas()\n","spacy_eng = spacy.load(\"en_core_web_sm\")\n","nltk.download(\"stopwords\")\n","lemm = WordNetLemmatizer()\n","init_notebook_mode(connected=True)\n","sns.set_style(\"darkgrid\")\n","plt.rcParams[\"figure.figsize\"] = (20, 8)\n","plt.rcParams[\"font.size\"] = 18"]},{"cell_type":"markdown","metadata":{},"source":["# Quora Question Semantic Similarity \n","\n","\n","**Semantic similarity** is a metric defined over a set of documents or terms, where the idea of distance between items is based on the likeness of their meaning or semantic content as opposed to lexicographical similarity. \n","- Semantic Similarity has various applications, such as information retrieval, text summarization, sentiment analysis, etc.\n","- For quora **information retrieval** serves an important purpose as users who post questions on the platform can/may find questions that are similar in meaning that have already been answered. Questions that are also semantically similar in nature can draw a user's attention to new content as well.\n","\n","<div class='alert alert-info'><strong>Note: </strong>Finding similarity semantically between sentences is different than finding similarity between sentences based on common keywords. Here the sentences in question need to have same meaning to be regarded as similar in nature.</div>"]},{"cell_type":"markdown","metadata":{},"source":["# Data Cleaning and EDA\n","- Cleaning and preprocessing text data\n","- Finding insights about sentence lengths and words present in them"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T15:48:24.466961Z","iopub.status.busy":"2022-11-04T15:48:24.465931Z","iopub.status.idle":"2022-11-04T15:48:25.776529Z","shell.execute_reply":"2022-11-04T15:48:25.775597Z","shell.execute_reply.started":"2022-11-04T15:48:24.466889Z"},"trusted":true},"outputs":[],"source":["data = pd.read_csv(\"train.csv\")\n","data.head(10)"]},{"cell_type":"markdown","metadata":{},"source":["## Remove Null Values"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T15:48:25.77926Z","iopub.status.busy":"2022-11-04T15:48:25.778945Z","iopub.status.idle":"2022-11-04T15:48:25.850134Z","shell.execute_reply":"2022-11-04T15:48:25.849118Z","shell.execute_reply.started":"2022-11-04T15:48:25.779221Z"},"trusted":true},"outputs":[],"source":["data.isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T15:48:25.852219Z","iopub.status.busy":"2022-11-04T15:48:25.851864Z","iopub.status.idle":"2022-11-04T15:48:25.98515Z","shell.execute_reply":"2022-11-04T15:48:25.984063Z","shell.execute_reply.started":"2022-11-04T15:48:25.852177Z"},"trusted":true},"outputs":[],"source":["data.dropna(inplace=True)"]},{"cell_type":"markdown","metadata":{},"source":["## Duplicate and Non-Duplicate Data Distribution\n","- Duplicate or similar questions are lesser in number which is to be expected as a platform for question answering will tend to have more unique questions in comparison to the questions that have been previously asked"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T15:48:25.986635Z","iopub.status.busy":"2022-11-04T15:48:25.986379Z","iopub.status.idle":"2022-11-04T15:48:29.070366Z","shell.execute_reply":"2022-11-04T15:48:29.06918Z","shell.execute_reply.started":"2022-11-04T15:48:25.986606Z"},"trusted":true},"outputs":[],"source":["fig = px.pie(\n","    data,\n","    values=\"id\",\n","    names=\"is_duplicate\",\n","    height=600,\n","    title=\"Proportion of Duplicate and Non Duplicate Questions\",\n",")\n","fig.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Text Cleaning\n","- Since the context of sentences are important for this NLP problem removal of stopwords might affect both the grammatical as well as semantic meaning of the sentences\n","- For similar reasons the words are not lemmatized or stemmed so that the semantic meaning of the sentence remains intact\n","- Therefore very basic cleaning is performed on the text data like removal of extra spaces and special characters"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T15:48:29.072575Z","iopub.status.busy":"2022-11-04T15:48:29.07224Z","iopub.status.idle":"2022-11-04T15:48:29.080839Z","shell.execute_reply":"2022-11-04T15:48:29.08011Z","shell.execute_reply.started":"2022-11-04T15:48:29.072539Z"},"trusted":true},"outputs":[],"source":["def text_cleaning(x):\n","\n","    questions = re.sub(\"\\s+\\n+\", \" \", x)\n","    questions = re.sub(\"[^a-zA-Z0-9]\", \" \", questions)\n","    questions = questions.lower()\n","\n","    return questions"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T15:48:29.082634Z","iopub.status.busy":"2022-11-04T15:48:29.08196Z","iopub.status.idle":"2022-11-04T15:48:36.333667Z","shell.execute_reply":"2022-11-04T15:48:36.332546Z","shell.execute_reply.started":"2022-11-04T15:48:29.082598Z"},"trusted":true},"outputs":[],"source":["data[\"question1_cleaned\"] = data[\"question1\"].progress_apply(text_cleaning)\n","data[\"question2_cleaned\"] = data[\"question2\"].progress_apply(text_cleaning)\n","data"]},{"cell_type":"markdown","metadata":{},"source":["## Sentence Length Distributions\n","- Here the objective is to find the ideal length of the sentence that should be used in our model\n","- In many cases the maximum sentence length is taken for embedding representations but by looking at the sentence length distributions a more informed decision can be made which will help in reducing the parameters of our model\n","- For transformers based models masks usually mask out the sentences which are short but are padded to a longer length, but since we are focused on finding the ideal length statistically we will go ahead with that"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T15:48:36.337121Z","iopub.status.busy":"2022-11-04T15:48:36.336758Z","iopub.status.idle":"2022-11-04T15:48:37.260379Z","shell.execute_reply":"2022-11-04T15:48:37.25952Z","shell.execute_reply.started":"2022-11-04T15:48:36.337077Z"},"trusted":true},"outputs":[],"source":["data[\"question1_lens\"] = data[\"question1_cleaned\"].apply(lambda x: len(x.split()))\n","data[\"question2_lens\"] = data[\"question2_cleaned\"].apply(lambda x: len(x.split()))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T15:48:37.26286Z","iopub.status.busy":"2022-11-04T15:48:37.262152Z","iopub.status.idle":"2022-11-04T15:48:41.307722Z","shell.execute_reply":"2022-11-04T15:48:41.306181Z","shell.execute_reply.started":"2022-11-04T15:48:37.262809Z"},"trusted":true},"outputs":[],"source":["px.histogram(\n","    data,\n","    x=\"question1_lens\",\n","    height=700,\n","    color=\"is_duplicate\",\n","    title=\"Question1 Length Distribution\",\n","    marginal=\"box\",\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T15:48:41.309484Z","iopub.status.busy":"2022-11-04T15:48:41.309228Z","iopub.status.idle":"2022-11-04T15:48:44.842354Z","shell.execute_reply":"2022-11-04T15:48:44.841618Z","shell.execute_reply.started":"2022-11-04T15:48:41.309455Z"},"trusted":true},"outputs":[],"source":["px.histogram(\n","    data,\n","    x=\"question2_lens\",\n","    height=700,\n","    color=\"is_duplicate\",\n","    title=\"Question2 Length Distribution\",\n","    marginal=\"box\",\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Word Cloud Visualization\n","- Word clouds help in visually identifying the most frequent words present in the sentences which also give a brief idea what the context of the sentences are\n","- Two wordclouds are visualized below for both pairs of sentences "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T15:48:44.844132Z","iopub.status.busy":"2022-11-04T15:48:44.843751Z","iopub.status.idle":"2022-11-04T15:48:44.863456Z","shell.execute_reply":"2022-11-04T15:48:44.862455Z","shell.execute_reply.started":"2022-11-04T15:48:44.844097Z"},"trusted":true},"outputs":[],"source":["question1 = data[\"question1_cleaned\"].tolist()\n","question2 = data[\"question2_cleaned\"].tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T15:48:44.865743Z","iopub.status.busy":"2022-11-04T15:48:44.86475Z","iopub.status.idle":"2022-11-04T15:49:00.787333Z","shell.execute_reply":"2022-11-04T15:49:00.786351Z","shell.execute_reply.started":"2022-11-04T15:48:44.865708Z"},"trusted":true},"outputs":[],"source":["wordcloud = WordCloud(max_words=1500, width=600, background_color=\"black\").generate(\n","    \" \".join(question1)\n",")\n","plt.imshow(wordcloud, interpolation=\"bilinear\")\n","plt.title(\"Words from Question1\")\n","plt.axis(\"off\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T15:49:00.788991Z","iopub.status.busy":"2022-11-04T15:49:00.788697Z","iopub.status.idle":"2022-11-04T15:49:16.435674Z","shell.execute_reply":"2022-11-04T15:49:16.434725Z","shell.execute_reply.started":"2022-11-04T15:49:00.788955Z"},"trusted":true},"outputs":[],"source":["wordcloud = WordCloud(max_words=1500, width=600, background_color=\"black\").generate(\n","    \" \".join(question2)\n",")\n","plt.imshow(wordcloud, interpolation=\"bilinear\")\n","plt.title(\"Words from Question2\")\n","plt.axis(\"off\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Ideal Sentence Length\n","- By looking at both the distrbution plots and descriptive statistics it is pretty clear that taking the maximum sentence length won't make much sense of our model\n","- The descriptive stats also represent the likeliness of an extremely long sentence really occuring on a platform like Quora\n","- Since the descriptive stats of both the pairs of questions look very similar lets analyse any one of them to find the upper outlier\n","- Once this upper outlier is found we can choose a number nearby to it to be our ideal sentence length for our embedding representation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T15:49:16.437099Z","iopub.status.busy":"2022-11-04T15:49:16.436836Z","iopub.status.idle":"2022-11-04T15:49:16.461315Z","shell.execute_reply":"2022-11-04T15:49:16.460418Z","shell.execute_reply.started":"2022-11-04T15:49:16.43707Z"},"trusted":true},"outputs":[],"source":["data[\"question1_lens\"].describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T15:49:16.462964Z","iopub.status.busy":"2022-11-04T15:49:16.462636Z","iopub.status.idle":"2022-11-04T15:49:16.486899Z","shell.execute_reply":"2022-11-04T15:49:16.485797Z","shell.execute_reply.started":"2022-11-04T15:49:16.462925Z"},"trusted":true},"outputs":[],"source":["data[\"question2_lens\"].describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T15:49:16.488325Z","iopub.status.busy":"2022-11-04T15:49:16.488026Z","iopub.status.idle":"2022-11-04T15:49:16.50789Z","shell.execute_reply":"2022-11-04T15:49:16.507023Z","shell.execute_reply.started":"2022-11-04T15:49:16.488296Z"},"trusted":true},"outputs":[],"source":["q1 = data[\"question1_lens\"].quantile(0.25)\n","q3 = data[\"question1_lens\"].quantile(0.75)\n","\n","upper_outlier = q3 + 1.5 * (q3 - q1)\n","print(upper_outlier)"]},{"cell_type":"markdown","metadata":{},"source":["**Inference:** Upper outlier is 22, lets take 50 to be the ideal length so that some of the extremely long sentences can also be represented well enough for our model"]},{"cell_type":"markdown","metadata":{},"source":["# Modelling\n","\n","For the purpose of performing semantic similarity we are going to use **Siamese Neural Networks**\n","\n","<div class='alert alert-info'><strong>Note: </strong>A <strong>Siamese Neural Network (SNN)</strong> is a class of neural network architectures that contain two or more identical sub-networks. “Identical” here means they have the same configuration with the same parameters and weights. These networks are used to find the similarity of the inputs by comparing their feature vectors.</div>\n","\n","They were termed as siamese networks due to the term Siamese twins, which comes from the twin conjoined brothers Chang and Eng Bunker who were born in Siam, now Thailand. \n","- The idea of having twin neural networks with same parameter and configurations is to extract features using the same setting for two different sentences\n","- Followed by this setting usually a distance layer is added to calculate the distance between the feature embeddings followed by dense layers and classification head\n","- For our problem statement we will use two variations of the siamese network\n","    - Original Siamese Network with L1 Distance Layer\n","    - Siamese Network with Triplet Loss\n","\n","<img src='https://img.freepik.com/free-photo/portrait-two-identical-siamese-cats_158595-5728.jpg?w=2000'>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T15:49:16.509535Z","iopub.status.busy":"2022-11-04T15:49:16.509235Z","iopub.status.idle":"2022-11-04T15:49:24.288729Z","shell.execute_reply":"2022-11-04T15:49:24.287844Z","shell.execute_reply.started":"2022-11-04T15:49:16.509507Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers import (\n","    Embedding,\n","    Layer,\n","    Dense,\n","    Dropout,\n","    MultiHeadAttention,\n","    LayerNormalization,\n","    Input,\n","    GlobalAveragePooling1D,\n",")\n","from tensorflow.keras.layers import LSTM, Bidirectional\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n","from sklearn.model_selection import train_test_split\n","from transformers import (\n","    AutoTokenizer,\n","    DataCollatorWithPadding,\n","    TFAutoModel,\n","    DistilBertConfig,\n","    TFDistilBertModel,\n","    BertConfig,\n","    TFBertModel,\n","    TFRobertaModel,\n",")\n","from datasets import load_dataset"]},{"cell_type":"markdown","metadata":{},"source":["## Siamese BERT\n","- For the backbone of our siamese neural network we will use a pretrained BERT model (since we required similar weights) \n","- BERT is an open source machine learning framework for natural language processing (NLP). BERT is designed to help computers understand the meaning of ambiguous language in text by using surrounding text to establish context.\n","- We will use just the encoder part of the BERT model for our problem\n","- The BERT variant that we are going to use is the base model\n","- Some other alternative of sequential models like RNN based models aren't used because they do not really have the concept of \"transfer learning\" in them and also they are computationally very expensive to train when it comes to attention mechanism.\n","\n","<img src='https://paul-hyun.github.io/assets/2020-01-02/bert-classification.png'>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T15:49:24.290329Z","iopub.status.busy":"2022-11-04T15:49:24.290087Z","iopub.status.idle":"2022-11-04T15:49:25.720308Z","shell.execute_reply":"2022-11-04T15:49:25.719153Z","shell.execute_reply.started":"2022-11-04T15:49:24.290301Z"},"trusted":true},"outputs":[],"source":["model_checkpoint = \"bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"]},{"cell_type":"markdown","metadata":{},"source":["## BERT Text Tokenizer\n","- Generates\n","    - Padded Encodings\n","    - Attention Masks"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T15:49:25.722545Z","iopub.status.busy":"2022-11-04T15:49:25.722218Z","iopub.status.idle":"2022-11-04T15:49:25.730873Z","shell.execute_reply":"2022-11-04T15:49:25.729738Z","shell.execute_reply.started":"2022-11-04T15:49:25.722501Z"},"trusted":true},"outputs":[],"source":["def encode_text(text, tokenizer):\n","\n","    encoded = tokenizer.batch_encode_plus(\n","        text,\n","        add_special_tokens=True,\n","        max_length=50,\n","        padding=\"max_length\",\n","        truncation=True,\n","        return_attention_mask=True,\n","        return_tensors=\"tf\",\n","    )\n","\n","    input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n","    attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n","\n","    return {\"input_ids\": input_ids, \"attention_masks\": attention_masks}"]},{"cell_type":"markdown","metadata":{},"source":["## Data Splitting\n","- 400000 data is sampled for our task\n","- 80:20 split is performed on the data\n","    - 80% for Training\n","    - 20% for Validation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T15:49:25.733197Z","iopub.status.busy":"2022-11-04T15:49:25.732813Z","iopub.status.idle":"2022-11-04T15:50:11.649094Z","shell.execute_reply":"2022-11-04T15:50:11.648306Z","shell.execute_reply.started":"2022-11-04T15:49:25.733154Z"},"trusted":true},"outputs":[],"source":["data = data.sample(400000)\n","train = data.iloc[: int(400000 * 0.80), :]\n","val = data.iloc[int(400000 * 0.80) :, :]\n","\n","X1_train = encode_text(train[\"question1_cleaned\"].tolist(), tokenizer)\n","X2_train = encode_text(train[\"question2_cleaned\"].tolist(), tokenizer)\n","X1_val = encode_text(val[\"question1_cleaned\"].tolist(), tokenizer)\n","X2_val = encode_text(val[\"question2_cleaned\"].tolist(), tokenizer)\n","\n","y_train = train[\"is_duplicate\"].values\n","y_val = val[\"is_duplicate\"].values"]},{"cell_type":"markdown","metadata":{},"source":["## TPU Configuration"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T15:50:11.650455Z","iopub.status.busy":"2022-11-04T15:50:11.650217Z","iopub.status.idle":"2022-11-04T15:50:17.554697Z","shell.execute_reply":"2022-11-04T15:50:17.553706Z","shell.execute_reply.started":"2022-11-04T15:50:11.650427Z"},"trusted":true},"outputs":[],"source":["try:\n","    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n","    tf.config.experimental_connect_to_cluster(tpu)\n","    tf.tpu.experimental.initialize_tpu_system(tpu)\n","    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n","    BATCH_SIZE = strategy.num_replicas_in_sync * 4\n","    print(\"Running on TPU:\", tpu.master())\n","    print(f\"Batch Size: {BATCH_SIZE}\")\n","\n","except ValueError:\n","    strategy = tf.distribute.get_strategy()\n","    BATCH_SIZE = 32\n","    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n","    print(f\"Batch Size: {BATCH_SIZE}\")"]},{"cell_type":"markdown","metadata":{},"source":["# Original Siamese Network with L1 Distance Layer\n","- This architecture uses the BERT twin backbone and applies the L1 distance on the embeddings returned by the backbone\n","- The L1 distance features are then fed to a dense layer to capture the non linearities\n","- Then the final layer is a sigmoid neuron which classfies whether the non linear activated distance features indicate if the sentences are similar or dissimilar\n","\n","<img src='https://www.frontiersin.org/files/Articles/839586/fbioe-10-839586-HTML/image_m/fbioe-10-839586-g002.jpg'>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T15:50:17.556279Z","iopub.status.busy":"2022-11-04T15:50:17.556026Z","iopub.status.idle":"2022-11-04T15:50:17.561378Z","shell.execute_reply":"2022-11-04T15:50:17.560412Z","shell.execute_reply.started":"2022-11-04T15:50:17.556251Z"},"trusted":true},"outputs":[],"source":["class L1Dist(Layer):\n","\n","    def __init__(self, **kwargs):\n","        super().__init__()\n","\n","    def call(self, embedding1, embedding2):\n","        return tf.math.abs(embedding1 - embedding2)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T15:50:17.566116Z","iopub.status.busy":"2022-11-04T15:50:17.56572Z","iopub.status.idle":"2022-11-04T15:50:56.005865Z","shell.execute_reply":"2022-11-04T15:50:56.004858Z","shell.execute_reply.started":"2022-11-04T15:50:17.566072Z"},"trusted":true},"outputs":[],"source":["with strategy.scope():\n","    transformer_model = TFBertModel.from_pretrained(model_checkpoint)\n","\n","    input_ids_in1 = Input(shape=(None,), name=\"input_ids1\", dtype=\"int32\")\n","    input_masks_in1 = Input(shape=(None,), name=\"attention_mask1\", dtype=\"int32\")\n","    input_ids_in2 = Input(shape=(None,), name=\"input_ids2\", dtype=\"int32\")\n","    input_masks_in2 = Input(shape=(None,), name=\"attention_mask2\", dtype=\"int32\")\n","\n","    embedding_layer1 = transformer_model(\n","        input_ids_in1, attention_mask=input_masks_in1\n","    ).last_hidden_state\n","    embedding_layer2 = transformer_model(\n","        input_ids_in2, attention_mask=input_masks_in2\n","    ).last_hidden_state\n","\n","    embedding1 = GlobalAveragePooling1D()(embedding_layer1)\n","    embedding2 = GlobalAveragePooling1D()(embedding_layer2)\n","    l1_dist = L1Dist()(embedding1, embedding2)\n","\n","    x = Dense(512, activation=\"relu\")(l1_dist)\n","    output = Dense(1, activation=\"sigmoid\")(x)\n","\n","    model = Model(\n","        inputs=[input_ids_in1, input_masks_in1, input_ids_in2, input_masks_in2],\n","        outputs=output,\n","    )\n","    model.compile(\n","        loss=\"binary_crossentropy\",\n","        optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001),\n","        metrics=\"accuracy\",\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T15:50:56.007735Z","iopub.status.busy":"2022-11-04T15:50:56.007359Z","iopub.status.idle":"2022-11-04T15:50:56.019898Z","shell.execute_reply":"2022-11-04T15:50:56.018803Z","shell.execute_reply.started":"2022-11-04T15:50:56.007692Z"},"trusted":true},"outputs":[],"source":["for layer in model.layers[:5]:\n","    layer.trainable = False"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T15:50:56.021476Z","iopub.status.busy":"2022-11-04T15:50:56.021179Z","iopub.status.idle":"2022-11-04T15:50:56.054231Z","shell.execute_reply":"2022-11-04T15:50:56.053058Z","shell.execute_reply.started":"2022-11-04T15:50:56.021445Z"},"trusted":true},"outputs":[],"source":["model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T15:50:56.05649Z","iopub.status.busy":"2022-11-04T15:50:56.056145Z","iopub.status.idle":"2022-11-04T15:50:56.062335Z","shell.execute_reply":"2022-11-04T15:50:56.061366Z","shell.execute_reply.started":"2022-11-04T15:50:56.056446Z"},"trusted":true},"outputs":[],"source":["from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T15:50:56.064541Z","iopub.status.busy":"2022-11-04T15:50:56.064205Z","iopub.status.idle":"2022-11-04T15:50:56.073878Z","shell.execute_reply":"2022-11-04T15:50:56.072468Z","shell.execute_reply.started":"2022-11-04T15:50:56.064499Z"},"trusted":true},"outputs":[],"source":["earlystopping = EarlyStopping(\n","    monitor=\"val_loss\", min_delta=0, patience=5, verbose=1, restore_best_weights=True\n",")\n","\n","learning_rate_reduction = ReduceLROnPlateau(\n","    monitor=\"val_loss\", patience=3, verbose=1, factor=0.3, min_lr=0.00000001\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T15:50:56.075478Z","iopub.status.busy":"2022-11-04T15:50:56.07518Z","iopub.status.idle":"2022-11-04T16:45:08.509544Z","shell.execute_reply":"2022-11-04T16:45:08.508503Z","shell.execute_reply.started":"2022-11-04T15:50:56.075437Z"},"trusted":true},"outputs":[],"source":["history = model.fit(\n","    (\n","        np.asarray(X1_train[\"input_ids\"]),\n","        np.asarray(X1_train[\"attention_masks\"]),\n","        np.asarray(X2_train[\"input_ids\"]),\n","        np.asarray(X2_train[\"attention_masks\"]),\n","    ),\n","    y_train,\n","    batch_size=BATCH_SIZE,\n","    epochs=5,\n","    validation_data=(\n","        (\n","            np.asarray(X1_val[\"input_ids\"]),\n","            np.asarray(X1_val[\"attention_masks\"]),\n","            np.asarray(X2_val[\"input_ids\"]),\n","            np.asarray(X2_val[\"attention_masks\"]),\n","        ),\n","        y_val,\n","    ),\n","    callbacks=[earlystopping, learning_rate_reduction],\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Model Inference\n","- Learning Curves\n","- ROC-AUC Curves\n","- Confusion Matrix\n","- Classification Report"]},{"cell_type":"markdown","metadata":{},"source":["## Learning Curve\n","- Since it is a pretrained model we do not train it for longer epochs, also due to the fact that the validation loss increases after first 3 epochs the training is restricted to 5 epochs\n","- The best weights from the most converged state are taken forward"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T16:46:02.834088Z","iopub.status.busy":"2022-11-04T16:46:02.833372Z","iopub.status.idle":"2022-11-04T16:46:03.152447Z","shell.execute_reply":"2022-11-04T16:46:03.151503Z","shell.execute_reply.started":"2022-11-04T16:46:02.834026Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(20, 8))\n","plt.plot(history.history[\"loss\"])\n","plt.plot(history.history[\"val_loss\"])\n","plt.title(\"model loss\")\n","plt.ylabel(\"loss\")\n","plt.xlabel(\"epoch\")\n","plt.legend([\"train\", \"val\"], loc=\"upper left\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T16:46:03.154513Z","iopub.status.busy":"2022-11-04T16:46:03.154274Z","iopub.status.idle":"2022-11-04T16:46:03.510633Z","shell.execute_reply":"2022-11-04T16:46:03.509873Z","shell.execute_reply.started":"2022-11-04T16:46:03.154484Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(20, 8))\n","plt.plot(history.history[\"accuracy\"])\n","plt.plot(history.history[\"val_accuracy\"])\n","plt.title(\"Model Accuracy\")\n","plt.ylabel(\"Accuracy\")\n","plt.xlabel(\"epoch\")\n","plt.legend([\"train\", \"val\"], loc=\"upper left\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T16:46:03.702629Z","iopub.status.busy":"2022-11-04T16:46:03.702297Z","iopub.status.idle":"2022-11-04T16:46:34.843292Z","shell.execute_reply":"2022-11-04T16:46:34.842155Z","shell.execute_reply.started":"2022-11-04T16:46:03.702595Z"},"trusted":true},"outputs":[],"source":["y_pred = model.predict(\n","    (\n","        np.asarray(X1_val[\"input_ids\"]),\n","        np.asarray(X1_val[\"attention_masks\"]),\n","        np.asarray(X2_val[\"input_ids\"]),\n","        np.asarray(X2_val[\"attention_masks\"]),\n","    )\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T16:46:34.845661Z","iopub.status.busy":"2022-11-04T16:46:34.845093Z","iopub.status.idle":"2022-11-04T16:46:34.850227Z","shell.execute_reply":"2022-11-04T16:46:34.849458Z","shell.execute_reply.started":"2022-11-04T16:46:34.845627Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import (\n","    classification_report,\n","    confusion_matrix,\n","    roc_auc_score,\n","    roc_curve,\n","    recall_score,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## ROC-AUC Curve\n","- The AUC score of 95% gives a clear indication about the good separability performance of our model \n","- The threshold values can be experimented with to acheive the desirable number of True positives or avoiding False positives"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T16:46:34.85215Z","iopub.status.busy":"2022-11-04T16:46:34.851319Z","iopub.status.idle":"2022-11-04T16:46:35.232643Z","shell.execute_reply":"2022-11-04T16:46:35.23167Z","shell.execute_reply.started":"2022-11-04T16:46:34.852117Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(20, 8))\n","fpr, tpr, _ = roc_curve(y_val, y_pred)\n","auc = roc_auc_score(y_val, y_pred)\n","plt.plot(fpr, tpr, label=\"CNN Model, auc=\" + str(auc), lw=2)\n","plt.plot([0, 1], [0, 1], color=\"orange\", lw=2, linestyle=\"--\")\n","plt.legend(loc=4)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Confusion Matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T16:46:35.234547Z","iopub.status.busy":"2022-11-04T16:46:35.23431Z","iopub.status.idle":"2022-11-04T16:46:35.808329Z","shell.execute_reply":"2022-11-04T16:46:35.807574Z","shell.execute_reply.started":"2022-11-04T16:46:35.234519Z"},"trusted":true},"outputs":[],"source":["y_pred[y_pred >= 0.5] = 1\n","y_pred[y_pred < 0.5] = 0\n","\n","sns.heatmap(\n","    confusion_matrix(y_val, y_pred),\n","    cmap=\"viridis\",\n","    annot=True,\n","    fmt=\".5g\",\n","    xticklabels=[\"Dissimilar\", \"Similar\"],\n","    yticklabels=[\"Dissimilar\", \"Similar\"],\n",")\n","plt.xlabel(\"Predicted Class\")\n","plt.ylabel(\"Actual Class\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Classification Report\n","- Our model achieves an F1-score of 89%\n","- F1-score is considered as there is a slight imbalance in the data\n","- Model performs slightlty less accurate for similar classes"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T16:46:35.810206Z","iopub.status.busy":"2022-11-04T16:46:35.809742Z","iopub.status.idle":"2022-11-04T16:46:35.990091Z","shell.execute_reply":"2022-11-04T16:46:35.989096Z","shell.execute_reply.started":"2022-11-04T16:46:35.810173Z"},"trusted":true},"outputs":[],"source":["print(classification_report(y_val, y_pred))"]},{"cell_type":"markdown","metadata":{},"source":["# Siamese Network with Triplet Loss\n","<div class='alert alert-info'><strong>Note: </strong>Using <strong>Triplet Loss</strong> we can train the network by taking an anchor text and comparing it with both a positive sample and a negative sample. The <strong>dissimilarity between the anchor text and positive text must be low</strong> and the <strong>dissimilarity between the anchor text and the negative text must be high</strong></div>\n","\n","The triplet loss is defined as:\n","\n","<img src='https://miro.medium.com/max/1328/1*nyfPmytStEZCijYl8OEAvQ.png'>\n","\n","- Triplet loss is a loss function for machine learning algorithms where a reference input is compared to a matching input and a non-matching input. The distance from the anchor to the positive is minimized, and the distance from the anchor to the negative input is maximized.\n","- The max and margin m make sure different points at distance > m do not contribute to the ranking loss. This has a significant advantage over contrastive loss.\n","\n","\n","<img src='https://miro.medium.com/max/1400/1*bvBns-k7sO2sNZE3fxWLFg.png'>"]},{"cell_type":"markdown","metadata":{},"source":["## Data Preparation\n","- The cleaned texts are rearranged and prepared in the anchor, positve and negative format"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T16:46:35.991567Z","iopub.status.busy":"2022-11-04T16:46:35.991317Z","iopub.status.idle":"2022-11-04T16:46:37.648281Z","shell.execute_reply":"2022-11-04T16:46:37.647288Z","shell.execute_reply.started":"2022-11-04T16:46:35.991538Z"},"trusted":true},"outputs":[],"source":["data = pd.read_csv(\"../input/triplet-data/triplet_data.csv\")\n","data.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T16:46:37.649596Z","iopub.status.busy":"2022-11-04T16:46:37.649365Z","iopub.status.idle":"2022-11-04T16:46:37.65584Z","shell.execute_reply":"2022-11-04T16:46:37.654884Z","shell.execute_reply.started":"2022-11-04T16:46:37.649566Z"},"trusted":true},"outputs":[],"source":["len(data)"]},{"cell_type":"markdown","metadata":{},"source":["## Data Splitting\n","- 140000 samples are taken for this model\n","- 80:20 splitting is performed\n","    - 80% taken for training\n","    - 20% taken for validation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T16:46:49.386098Z","iopub.status.busy":"2022-11-04T16:46:49.385779Z","iopub.status.idle":"2022-11-04T16:47:15.909215Z","shell.execute_reply":"2022-11-04T16:47:15.908155Z","shell.execute_reply.started":"2022-11-04T16:46:49.386068Z"},"trusted":true},"outputs":[],"source":["train = data.iloc[: int(140000 * 0.80), :]\n","val = data.iloc[int(140000 * 0.80) :, :]\n","\n","X1_train = encode_text(train[\"question1_cleaned\"].tolist(), tokenizer)\n","X2_train = encode_text(train[\"question2_cleaned\"].tolist(), tokenizer)\n","X3_train = encode_text(train[\"question3_cleaned\"].tolist(), tokenizer)\n","\n","X1_val = encode_text(val[\"question1_cleaned\"].tolist(), tokenizer)\n","X2_val = encode_text(val[\"question2_cleaned\"].tolist(), tokenizer)\n","X3_val = encode_text(val[\"question3_cleaned\"].tolist(), tokenizer)"]},{"cell_type":"markdown","metadata":{},"source":["## Custom Distance Layer\n","- calculates distane between anchor and positive and anchor and negative"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T16:47:15.911206Z","iopub.status.busy":"2022-11-04T16:47:15.910898Z","iopub.status.idle":"2022-11-04T16:47:15.916996Z","shell.execute_reply":"2022-11-04T16:47:15.91606Z","shell.execute_reply.started":"2022-11-04T16:47:15.911173Z"},"trusted":true},"outputs":[],"source":["class DistanceLayer(Layer):\n","\n","    def __init__(self, **kwargs):\n","        super().__init__(**kwargs)\n","\n","    def call(self, anchor, positive, negative):\n","        ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)\n","        an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)\n","        return (ap_distance, an_distance)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T16:47:15.919007Z","iopub.status.busy":"2022-11-04T16:47:15.918356Z","iopub.status.idle":"2022-11-04T16:47:15.929031Z","shell.execute_reply":"2022-11-04T16:47:15.927983Z","shell.execute_reply.started":"2022-11-04T16:47:15.918961Z"},"trusted":true},"outputs":[],"source":["from tensorflow.keras import metrics"]},{"cell_type":"markdown","metadata":{},"source":["## Custom Model\n","- Takes the siamese network architecture as an input and optimizes it with respect to the triplet loss"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T16:47:15.931571Z","iopub.status.busy":"2022-11-04T16:47:15.931195Z","iopub.status.idle":"2022-11-04T16:47:15.943689Z","shell.execute_reply":"2022-11-04T16:47:15.942532Z","shell.execute_reply.started":"2022-11-04T16:47:15.931526Z"},"trusted":true},"outputs":[],"source":["class SiameseModel(Model):\n","\n","    def __init__(self, siamese_network, margin=0.5):\n","        super(SiameseModel, self).__init__()\n","        self.siamese_network = siamese_network\n","        self.margin = margin\n","        self.loss_tracker = metrics.Mean(name=\"loss\")\n","\n","    def call(self, inputs):\n","        return self.siamese_network(inputs)\n","\n","    def train_step(self, data):\n","\n","        with tf.GradientTape() as tape:\n","            loss = self._compute_loss(data)\n","\n","        gradients = tape.gradient(loss, self.siamese_network.trainable_weights)\n","        self.optimizer.apply_gradients(\n","            zip(gradients, self.siamese_network.trainable_weights)\n","        )\n","        self.loss_tracker.update_state(loss)\n","        return {\"loss\": self.loss_tracker.result()}\n","\n","    def test_step(self, data):\n","        loss = self._compute_loss(data)\n","        self.loss_tracker.update_state(loss)\n","        return {\"loss\": self.loss_tracker.result()}\n","\n","    def _compute_loss(self, data):\n","        ap_distance, an_distance = self.siamese_network(data)\n","        loss = ap_distance - an_distance\n","        loss = tf.maximum(loss + self.margin, 0.0)\n","        return loss\n","\n","    @property\n","    def metrics(self):\n","        return [self.loss_tracker]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T16:47:15.945679Z","iopub.status.busy":"2022-11-04T16:47:15.945307Z","iopub.status.idle":"2022-11-04T17:02:23.788553Z","shell.execute_reply":"2022-11-04T17:02:23.787634Z","shell.execute_reply.started":"2022-11-04T16:47:15.945624Z"},"trusted":true},"outputs":[],"source":["with strategy.scope():\n","    transformer_model = TFBertModel.from_pretrained(model_checkpoint)\n","\n","    input_ids_in1 = Input(shape=(50,), name=\"input_ids1\", dtype=\"int32\")\n","    input_masks_in1 = Input(shape=(50,), name=\"attention_mask1\", dtype=\"int32\")\n","\n","    anchor_input = Input(name=\"anchor_ids\", shape=(50,), dtype=\"int32\")\n","    anchor_masks = Input(name=\"anchor_mask\", shape=(50,), dtype=\"int32\")\n","\n","    positive_input = Input(name=\"positive_ids\", shape=(50,), dtype=\"int32\")\n","    positive_masks = Input(name=\"positive_mask\", shape=(50,), dtype=\"int32\")\n","\n","    negative_input = Input(name=\"negative_ids\", shape=(50,), dtype=\"int32\")\n","    negative_masks = Input(name=\"negative_mask\", shape=(50,), dtype=\"int32\")\n","\n","    embedding_layer = transformer_model(\n","        input_ids_in1, attention_mask=input_masks_in1\n","    ).last_hidden_state\n","\n","    average = GlobalAveragePooling1D()(embedding_layer)\n","    embeds = Dense(512, activation=\"relu\")(average)\n","\n","    embeddings = Model(inputs=[input_ids_in1, input_masks_in1], outputs=embeds)\n","\n","    for layer in embeddings.layers[:-1]:\n","        layer.trainable = False\n","\n","    embeds1 = embeddings([anchor_input, anchor_masks])\n","    embeds2 = embeddings([positive_input, positive_masks])\n","    embeds3 = embeddings([negative_input, negative_masks])\n","\n","    distances = DistanceLayer()(embeds1, embeds2, embeds3)\n","\n","    siamese_network = Model(\n","        inputs=[\n","            anchor_input,\n","            anchor_masks,\n","            positive_input,\n","            positive_masks,\n","            negative_input,\n","            negative_masks,\n","        ],\n","        outputs=distances,\n","    )\n","\n","    siamese_model = SiameseModel(siamese_network)\n","    siamese_model.compile(optimizer=tf.keras.optimizers.Adam(0.00001))\n","    history = siamese_model.fit(\n","        (\n","            np.asarray(X1_train[\"input_ids\"]),\n","            np.asarray(X1_train[\"attention_masks\"]),\n","            np.asarray(X2_train[\"input_ids\"]),\n","            np.asarray(X2_train[\"attention_masks\"]),\n","            np.asarray(X3_train[\"input_ids\"]),\n","            np.asarray(X3_train[\"attention_masks\"]),\n","        ),\n","        epochs=10,\n","        validation_data=(\n","            (\n","                np.asarray(X1_val[\"input_ids\"]),\n","                np.asarray(X1_val[\"attention_masks\"]),\n","                np.asarray(X2_val[\"input_ids\"]),\n","                np.asarray(X2_val[\"attention_masks\"]),\n","                np.asarray(X3_val[\"input_ids\"]),\n","                np.asarray(X3_val[\"attention_masks\"]),\n","            ),\n","        ),\n","    )"]},{"cell_type":"markdown","metadata":{},"source":["# Model Inference\n","- Learning Curve\n","- Cosine Similarity between Embeddings"]},{"cell_type":"markdown","metadata":{},"source":["## Learning Curve\n","- The model converges well, but the validation loss doesn't seem to be close to the training loss"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T17:11:30.138817Z","iopub.status.busy":"2022-11-04T17:11:30.138501Z","iopub.status.idle":"2022-11-04T17:11:30.471132Z","shell.execute_reply":"2022-11-04T17:11:30.469923Z","shell.execute_reply.started":"2022-11-04T17:11:30.138786Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(20, 8))\n","plt.plot(history.history[\"loss\"])\n","plt.plot(history.history[\"val_loss\"])\n","plt.title(\"model loss\")\n","plt.ylabel(\"loss\")\n","plt.xlabel(\"epoch\")\n","plt.legend([\"train\", \"val\"], loc=\"upper left\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Cosine Similarity between Embeddings\n","\n","<div class='alert alert-info'><strong>Note: </strong>The Cosine Similarity is a better metric than Euclidean distance for text similarity because if the two text document far apart by Euclidean distance, there are still chances that they are close to each other in terms of their context.</div>\n","\n","<img src='https://developers.google.com/machine-learning/crash-course/images/linear-relationships.svg'>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T17:11:35.953039Z","iopub.status.busy":"2022-11-04T17:11:35.952704Z","iopub.status.idle":"2022-11-04T17:11:35.96085Z","shell.execute_reply":"2022-11-04T17:11:35.959728Z","shell.execute_reply.started":"2022-11-04T17:11:35.953004Z"},"trusted":true},"outputs":[],"source":["def get_cosine_similarity(sentence1, sentence2):\n","\n","    x1 = text_cleaning(sentence1)\n","    x1 = encode_text([x1], tokenizer)\n","    x2 = text_cleaning(sentence2)\n","    x2 = encode_text([x2], tokenizer)\n","\n","    x1_inputs = np.array(x1[\"input_ids\"])\n","    x1_masks = np.array(x1[\"attention_masks\"])\n","    x2_inputs = np.array(x2[\"input_ids\"])\n","    x2_masks = np.array(x2[\"attention_masks\"])\n","\n","    embeddings1 = embeddings([x1_inputs, x1_masks])\n","    embeddings2 = embeddings([x2_inputs, x2_masks])\n","\n","    cosine_similarity = metrics.CosineSimilarity()\n","\n","    return cosine_similarity(embeddings1, embeddings2).numpy()"]},{"cell_type":"markdown","metadata":{},"source":["## Sample Test Cases\n","- Cosine Similarity ranges from 0 to 1. \n","- Value closer to 1 indicates higher similarity and a value closer to 0 indicates dissimilarity"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T17:11:54.495266Z","iopub.status.busy":"2022-11-04T17:11:54.49495Z","iopub.status.idle":"2022-11-04T17:11:56.058376Z","shell.execute_reply":"2022-11-04T17:11:56.057574Z","shell.execute_reply.started":"2022-11-04T17:11:54.495232Z"},"trusted":true},"outputs":[],"source":["sentence1 = \"Is Earth circle in shape ?\"\n","sentence2 = \"Should I learn python as it is very popular ?\"\n","get_cosine_similarity(sentence1, sentence2)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T17:12:22.679841Z","iopub.status.busy":"2022-11-04T17:12:22.678956Z","iopub.status.idle":"2022-11-04T17:12:24.234284Z","shell.execute_reply":"2022-11-04T17:12:24.233348Z","shell.execute_reply.started":"2022-11-04T17:12:22.679794Z"},"trusted":true},"outputs":[],"source":["sentence1 = \"Python is one of the most popular programming language out there\"\n","sentence2 = \"Should I learn python programming as it is very popular ?\"\n","get_cosine_similarity(sentence1, sentence2)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T17:13:43.118305Z","iopub.status.busy":"2022-11-04T17:13:43.117824Z","iopub.status.idle":"2022-11-04T17:13:44.695488Z","shell.execute_reply":"2022-11-04T17:13:44.694556Z","shell.execute_reply.started":"2022-11-04T17:13:43.118254Z"},"trusted":true},"outputs":[],"source":["sentence1 = \"Which GPU gives a better performance NVIDIA or AMD ?\"\n","sentence2 = \"What is the recipe for Kolkata Chicken Roll?\"\n","get_cosine_similarity(sentence1, sentence2)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T17:14:22.01946Z","iopub.status.busy":"2022-11-04T17:14:22.01874Z","iopub.status.idle":"2022-11-04T17:14:23.664363Z","shell.execute_reply":"2022-11-04T17:14:23.663214Z","shell.execute_reply.started":"2022-11-04T17:14:22.019416Z"},"trusted":true},"outputs":[],"source":["sentence1 = \"Which GPU gives a better performance NVIDIA or AMD ?\"\n","sentence2 = \"My friend has a NVIDIA GPU, and he suggests that it gives a very smooth gaming performance\"\n","get_cosine_similarity(sentence1, sentence2)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T17:15:31.569921Z","iopub.status.busy":"2022-11-04T17:15:31.569547Z","iopub.status.idle":"2022-11-04T17:15:33.163966Z","shell.execute_reply":"2022-11-04T17:15:33.162726Z","shell.execute_reply.started":"2022-11-04T17:15:31.56987Z"},"trusted":true},"outputs":[],"source":["sentence1 = \"Which GPU gives a better performance NVIDIA or AMD ?\"\n","sentence2 = \"NVIDIA manufactures the best performing GPUS\"\n","get_cosine_similarity(sentence1, sentence2)"]},{"cell_type":"markdown","metadata":{},"source":["<div class='alert alert-success'><strong>Conclusion:</strong>\n","    <li>Both the architectures seem to be performing well in terms of their inferences</li>\n","    <li>Original Siamese network can be evalutated in terms of classification metrics and similar sentences can be found using <strong>probability scores</strong></li>\n","    <li>Siamese network with triplet loss can be evaluated in terms of cosine distances and similar sentences can be found using <strong>higher cosine distances</strong></li>\n","    <li>For a real time scenario a metric like <strong>Hit Rate or User Engagement</strong> will prove to be more useful to get an infication of the usability of the model</li>\n","</div>"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":323734,"sourceId":6277,"sourceType":"competition"},{"datasetId":2598141,"sourceId":4437011,"sourceType":"datasetVersion"},{"datasetId":2603678,"sourceId":4446298,"sourceType":"datasetVersion"}],"dockerImageVersionId":30299,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
